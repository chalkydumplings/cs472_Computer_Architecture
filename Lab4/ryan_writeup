The article begins with the following problem: CPU speeds are improving at a much faster rate than memory speeds. The burden of trying to avoid this blatant memory bottleneck then falls on the cache.

Next, the cache is briefly explained by way of it's component parts, the possible mappings, the hierarchy, and some typical specs. There are 3 types of cache misses: compulsory, capacity, and conflict. But there are 3 techniques that can be used to help avoid these: rearrange, reduce and reuse.

How can you make sure your code is optimized to use cache efficiently? Well there is a pretty extensive list of practices/techniques that will help. Encapsulation and the object oriented programming pattern will both typically hurt cache performance, but monolithic function on the other hand, will help. Another import factor is code size. If you can rewrite portions of your code in asm, there is a good chance that your version will be smaller than the compiled version. You should also avoid inlining, unrolling, and large macros. Additionally, compressing data can help and you can also pad your data to ensure that it aligns to cache lines.

Prefetching and preloading are two powerful techniques that, when used properly, will enable your code to better utilize the cache. The goal of prefetching is to get the `timing' right, so that data is available exactly when needed (not too early or too late). Preloading is the process of loading multiple pieces of data into the cache and then processing it, rather than alternating load, process, load process. Loops will typically do the latter rather than the former if you aren't careful.

It's also important to keep in mind structures and the layout of your code. If two variables are commonly used together, well then store them in a struct; and you can even improve on that by placing the fields/variables right after one another to ensure that they will be stored contiguously in memory. Another related technique is hot/cold splitting. Also, if a struct contains many variables, note the sizes used, use padding if necessary, or use a decreasing size structure, from largest variable types at the top, to smallest variables types at the bottom.

When using tree data structures, there are a lot of options for how your can rearrange ndoes and reduce the size. The proper order for storing the data that constitutes a tree really depends on how you are typically going to be accessing that data - i.e. will you more commonly do a breadth-first or depth-first traversal? One of the best things you can do is to linearize your data at runtime - it gives you the best possible spatial locality and it makes data easily prefetchable.

Aliasing is multiple references to the same storage location and it causes all sorts of optimization problems via poisoning the data cache, negatively affecting instruction scheduling, etc. Better languages and compilers can help, but there are things you can do as a programmer, too. For example, when you are unrolling, if you consume all inputs before producing outputs, you can reduce refetches.

Another pretty general problem is that higher levels of abstraction nave a negative effect on optimization. This is true for C++ (And I assume this applies to many other languages, too).

Some additional general tips for avoiding aliasing include: minimizing use of globals, pointers, references (by passing small variables by value); use local variables as much as possible; and don't take the address of variables. You can also use fstrict aliasing in gcc to help you avoid certain aliasing issues.

How will you know if your optimizations are effective?

One of the best things you can do is study the generated code, and this will help you build intuition about how to write code that produces simpler output (when compiled to asm). You can also profile your cache utilization using a number of different commercial products, but a good place to start is to just use gcc with the p flag and mcount.
